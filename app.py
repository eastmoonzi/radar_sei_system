import streamlit as st
import pandas as pd
import os
import time
from typing import Tuple # ç¡®ä¿ typing è¢«å¯¼å…¥

# å¯¼å…¥æˆ‘ä»¬æ‰€æœ‰çš„è‡ªå®šä¹‰æ¨¡å—
try:
    from radar_sei_system.data_management import load_iq_data
    from radar_sei_system.feature_extraction import extract_features
    from radar_sei_system.ml_modeling import train, predict
    from radar_sei_system.performance_evaluation import evaluate
except ImportError as e:
    st.error(f"å¯åŠ¨å¤±è´¥ï¼šæ— æ³•å¯¼å…¥æ ¸å¿ƒæ¨¡å—ã€‚è¯·æ£€æŸ¥ __init__.py æ–‡ä»¶æ˜¯å¦é…ç½®æ­£ç¡®ã€‚")
    st.error(f"è¯¦ç»†é”™è¯¯: {e}")
    st.stop()


# --- 1. åŸºæœ¬é…ç½®å’Œå¸¸é‡ ---
# ==================================================================
# ================== è¯Šæ–­ä¿®æ”¹ç‚¹ï¼šä¿®æ”¹æ ‡é¢˜ ==================
st.set_page_config(page_title="é›·è¾¾è¾å°„æºè¯†åˆ«ç³»ç»Ÿ (V3)", layout="wide")
st.title("âœ…ã€V3 å¼ºåˆ¶åˆ·æ–°ç‰ˆã€‘é›·è¾¾è¾å°„æºæŒ‡çº¹è¯†åˆ«ç³»ç»Ÿ")
# ==================================================================
# ==================================================================

# ç¡¬ç¼–ç æ¨¡å‹è·¯å¾„
MODEL_SAVE_DIR = "./saved_models"
MODEL_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, "mvp_model.pkl")

# ä¸´æ—¶çš„æ–‡ä»¶å¤„ç†ç›®å½•
TEMP_DIR = "./temp_uploads"
if not os.path.exists(TEMP_DIR):
    os.makedirs(TEMP_DIR)

# --- 2. è¾…åŠ©å‡½æ•° ---
def save_uploaded_file(uploaded_file):
    """ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•ï¼Œè¿”å›æ–‡ä»¶è·¯å¾„"""
    temp_path = os.path.join(TEMP_DIR, uploaded_file.name)
    with open(temp_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    return temp_path

def cleanup_temp_file(file_path):
    """åˆ é™¤ä¸´æ—¶æ–‡ä»¶"""
    if os.path.exists(file_path):
        os.remove(file_path)

# --- 3. é¡µé¢å¯¼èˆª (ä¾§è¾¹æ ) ---
st.sidebar.title("å¯¼èˆª")
page = st.sidebar.radio("é€‰æ‹©åŠŸèƒ½", ["ğŸ¯ é¢„æµ‹ (Prediction)", "ğŸ‹ï¸ è®­ç»ƒ (Training)"])

# ==============================================================================
# --- é¡µé¢ä¸€ï¼šé¢„æµ‹ ---
# ==============================================================================
if page == "ğŸ¯ é¢„æµ‹ (Prediction)":
    st.header("ğŸ¯ è¾å°„æºèº«ä»½é¢„æµ‹")

    # 1. æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(MODEL_SAVE_PATH):
        st.error(f"è­¦å‘Šï¼šæœªæ‰¾åˆ°å·²è®­ç»ƒçš„æ¨¡å‹ï¼ (åº”ä½äº: {MODEL_SAVE_PATH})")
        st.info("è¯·å…ˆå» 'ğŸ‹ï¸ è®­ç»ƒ (Training)' é¡µé¢è®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚")
        st.stop()
    
    st.success(f"å·²åŠ è½½æ¨¡å‹: {MODEL_SAVE_PATH}")

    # --- (V2 æ–°åŠŸèƒ½) é¢„æµ‹æ—¶ä¹Ÿéœ€è¦é€‰æ‹©ç‰¹å¾ ---
    st.subheader("1. ç‰¹å¾é€‰æ‹©")
    st.warning("è¯·ç¡®ä¿ä½ é€‰æ‹©çš„ç‰¹å¾ä¸è®­ç»ƒæ¨¡å‹æ—¶ä½¿ç”¨çš„ç‰¹å¾ *å®Œå…¨ä¸€è‡´*ï¼")
    feature_options = st.multiselect(
        'é€‰æ‹©ç”¨äºé¢„æµ‹çš„ç‰¹å¾:',
        ['power_spectrum', 'vmd'],
        default=['power_spectrum'] # é»˜è®¤å€¼
    )
    # ------------------------------------
    
    # 2. æ–‡ä»¶ä¸Šä¼ 
    st.subheader("2. ä¸Šä¼ æ•°æ®")
    uploaded_file = st.file_uploader("ä¸Šä¼ ä¸€ä¸ª .h5 æ–‡ä»¶è¿›è¡Œé¢„æµ‹", type=["h5"])

    if uploaded_file is not None:
        st.write(f"å·²ä¸Šä¼ æ–‡ä»¶: `{uploaded_file.name}`")
        
        # 3. å¼€å§‹é¢„æµ‹
        if st.button("å¼€å§‹é¢„æµ‹"):
            if not feature_options:
                st.error("è¯·è‡³å°‘é€‰æ‹©ä¸€ç§ç‰¹å¾ï¼")
                st.stop()

            with st.spinner('æ­£åœ¨å¤„ç†...'):
                temp_file_path = ""
                try:
                    # æ­¥éª¤ A: ä¿å­˜ä¸´æ—¶æ–‡ä»¶
                    temp_file_path = save_uploaded_file(uploaded_file)
                    
                    # æ­¥éª¤ B: åŠ è½½æ•°æ®
                    st.subheader("A. æ•°æ®åŠ è½½")
                    data_obj = load_iq_data(temp_file_path)
                    if not data_obj:
                        st.error("æ•°æ®åŠ è½½å¤±è´¥ï¼")
                        st.stop()
                    st.write(f"ä¿¡å·é•¿åº¦: {len(data_obj['iq_data'])}, é‡‡æ ·ç‡: {data_obj['sampling_rate']/1e6} MHz")

                    # æ­¥éª¤ C: æå–ç‰¹å¾ (ä½¿ç”¨é€‰æ‹©çš„ç‰¹å¾)
                    st.subheader("B. ç‰¹å¾æå–")
                    feature_obj = extract_features(data_obj, methods=feature_options)
                    if feature_obj.empty:
                        st.error("ç‰¹å¾æå–å¤±è´¥ï¼")
                        st.stop()
                    st.dataframe(feature_obj)

                    # æ­¥éª¤ D: æ‰§è¡Œé¢„æµ‹
                    st.subheader("C. é¢„æµ‹ç»“æœ")
                    prediction_list = predict(feature_obj, MODEL_SAVE_PATH)
                    if prediction_list:
                        result = prediction_list[0]
                        st.metric(label="é¢„æµ‹æ ‡ç­¾", value=result.get('predicted_label'))
                        st.json(result.get('probabilities'))
                    else:
                        st.error("é¢„æµ‹æ‰§è¡Œå¤±è´¥ï¼è¯·æ£€æŸ¥æ¨¡å‹ä¸ç‰¹å¾æ˜¯å¦åŒ¹é…ã€‚")
                        st.stop()

                except Exception as e:
                    st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
                
                finally:
                    # æ­¥éª¤ E: æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    cleanup_temp_file(temp_file_path)
                    
            st.success("é¢„æµ‹å®Œæˆï¼")

# ==============================================================================
# --- é¡µé¢äºŒï¼šè®­ç»ƒ ---
# ==============================================================================
elif page == "ğŸ‹ï¸ è®­ç»ƒ (Training)":
    st.header("ğŸ‹ï¸ è®­ç»ƒæ–°æ¨¡å‹")
    st.info("""
    **æç¤º:**
    1.  ä¸Šä¼ çš„æ–‡ä»¶å¿…é¡»åŒ…å«**è‡³å°‘2ä¸ªä¸åŒ**çš„å†…éƒ¨æ ‡ç­¾æ‰èƒ½è®­ç»ƒã€‚
    2.  ç¨‹åºå°†è‡ªåŠ¨ä» .h5 æ–‡ä»¶å†…éƒ¨çš„ `InterPulse/LABEL` è·¯å¾„è¯»å–æ ‡ç­¾ã€‚
    """)

    # --- (V2 æ–°åŠŸèƒ½) è®­ç»ƒæ—¶é€‰æ‹©ç‰¹å¾ ---
    st.subheader("1. ç‰¹å¾é€‰æ‹©")
    feature_options = st.multiselect(
        'é€‰æ‹©è¦æå–çš„ç‰¹å¾ (å¯å¤šé€‰):',
        ['power_spectrum', 'vmd'], # 'vmd' æ˜¯æˆ‘ä»¬æ–°å¢çš„é€‰é¡¹
        default=['power_spectrum'] # é»˜è®¤åªé€‰æˆ‘ä»¬ä¹‹å‰é‚£ä¸ª
    )
    # -----------------------------

    # 1. æ–‡ä»¶ä¸Šä¼ 
    st.subheader("2. ä¸Šä¼ æ•°æ®")
    uploaded_files = st.file_uploader("ä¸Šä¼ è®­ç»ƒæ•°æ®é›† (å¯å¤šé€‰)", accept_multiple_files=True, type=["h5"])

    if uploaded_files:
        st.write(f"æ€»å…±ä¸Šä¼ äº† {len(uploaded_files)} ä¸ªæ–‡ä»¶ã€‚")
        
        # 2. å¼€å§‹è®­ç»ƒ
        if st.button("å¼€å§‹è®­ç»ƒ"):
            if not feature_options:
                st.error("è¯·è‡³å°‘é€‰æ‹©ä¸€ç§ç‰¹å¾ï¼")
                st.stop()

            if len(uploaded_files) < 2:
                st.error("è®­ç»ƒè‡³å°‘éœ€è¦2ä¸ªæ–‡ä»¶ã€‚")
                st.stop()

            all_features_list = []
            all_labels = []
            temp_files_to_clean = []
            
            with st.spinner(f'æ­£åœ¨å¤„ç† {len(uploaded_files)} ä¸ªæ–‡ä»¶... VMDå¯èƒ½å¾ˆæ…¢...'):
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                for i, file in enumerate(uploaded_files):
                    status_text.text(f"æ­£åœ¨å¤„ç†: {file.name}...")
                    try:
                        # æ­¥éª¤ A: ä¿å­˜ä¸´æ—¶æ–‡ä»¶
                        temp_path = save_uploaded_file(file)
                        temp_files_to_clean.append(temp_path)
                        
                        # æ­¥éª¤ B: åŠ è½½æ•°æ® (loader.py ä¼šè‡ªåŠ¨æå–å†…éƒ¨æ ‡ç­¾)
                        data_obj = load_iq_data(temp_path)
                        if not data_obj:
                            st.warning(f"åŠ è½½ {file.name} å¤±è´¥ï¼Œå·²è·³è¿‡ã€‚")
                            continue
                            
                        # æ­¥éª¤ C: æå–æ ‡ç­¾ (ä» data_obj ä¸­è·å–)
                        label = data_obj.get("label")
                        if not label or label == "unknown":
                            st.warning(f"æ–‡ä»¶ {file.name} å†…éƒ¨æœªæ‰¾åˆ°æœ‰æ•ˆæ ‡ç­¾ï¼Œå·²è·³è¿‡ã€‚")
                            continue
                            
                        # æ­¥éª¤ D: æå–ç‰¹å¾ (ä½¿ç”¨é€‰æ‹©çš„ç‰¹å¾)
                        feature_obj = extract_features(data_obj, methods=feature_options)
                        if feature_obj.empty:
                            st.warning(f"æå– {file.name} ç‰¹å¾å¤±è´¥ï¼Œå·²è·³è¿‡ã€‚")
                            continue
                            
                        all_features_list.append(feature_obj)
                        all_labels.append(label)
                        
                    except Exception as e:
                        st.warning(f"å¤„ç† {file.name} æ—¶å‡ºé”™: {e}ï¼Œå·²è·³è¿‡ã€‚")
                    
                    progress_bar.progress((i + 1) / len(uploaded_files))
                
                status_text.text("æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæ¯•ï¼")

            # æ­¥éª¤ E: åˆå¹¶æ‰€æœ‰ç‰¹å¾
            if not all_features_list:
                st.error("æ²¡æœ‰æ–‡ä»¶è¢«æˆåŠŸå¤„ç†ï¼è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œå†…å®¹ã€‚")
                st.stop()
                
            training_features = pd.concat(all_features_list, ignore_index=True)
            st.subheader("æå–çš„ç‰¹å¾æ€»è§ˆ (å‰5è¡Œ):")
            st.dataframe(training_features.head())
            
            label_counts = pd.Series(all_labels).value_counts()
            st.write(f"æ€»å…±æå–äº† {len(all_labels)} ä¸ªæ ·æœ¬ã€‚")
            st.write("æ ‡ç­¾åˆ†å¸ƒ:")
            st.dataframe(label_counts)

            if len(label_counts) < 2:
                st.error(f"è®­ç»ƒå¤±è´¥ï¼šåªæ‰¾åˆ°äº† {len(label_counts)} ä¸ªå”¯ä¸€çš„æ ‡ç­¾ã€‚åˆ†ç±»å™¨è‡³å°‘éœ€è¦2ä¸ªä¸åŒçš„ç±»åˆ«æ‰èƒ½è®­ç»ƒã€‚")
                st.stop()

            # æ­¥éª¤ F: æ‰§è¡Œè®­ç»ƒ
            st.subheader("æ¨¡å‹è®­ç»ƒ")
            with st.spinner("æ­£åœ¨è®­ç»ƒæ¨¡å‹..."):
                model_path, train_log = train(training_features, all_labels, 
                                              model_type="mvp_logistic", params={})
            
            st.success(f"è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
            st.json(train_log)

            # æ­¥éª¤ G: åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œè¯„ä¼°
            st.subheader("è®­ç»ƒé›†è¡¨ç° (ç”¨äºè°ƒè¯•)")
            predictions_on_train = predict(training_features, model_path)
            eval_results = evaluate(predictions_on_train, all_labels)
            
            st.metric(label="è®­ç»ƒé›†å‡†ç¡®ç‡", value=f"{eval_results.get('accuracy', 0):.2%}")
            
            st.write("æ··æ·†çŸ©é˜µ:")
            cm_labels = eval_results.get('labels_in_matrix')
            if cm_labels:
                st.dataframe(pd.DataFrame(eval_results.get('confusion_matrix'), 
                                         columns=cm_labels, 
                                         index=cm_labels))
            else:
                st.write("æ— æ³•ç”Ÿæˆæ··æ·†çŸ©é˜µã€‚")

            # æ­¥éª¤ H: æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
            for path in temp_files_to_clean:
                cleanup_temp_file(path)
            st.info("ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†ã€‚")