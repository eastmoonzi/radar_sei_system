import streamlit as st
import pandas as pd
import os
import time

# å¯¼å…¥æˆ‘ä»¬æ‰€æœ‰çš„è‡ªå®šä¹‰æ¨¡å—
from radar_sei_system.data_management import load_iq_data
from radar_sei_system.feature_extraction import extract_features
from radar_sei_system.ml_modeling import train, predict
from radar_sei_system.performance_evaluation import evaluate

# --- 1. åŸºæœ¬é…ç½®å’Œå¸¸é‡ ---
st.set_page_config(page_title="é›·è¾¾è¾å°„æºè¯†åˆ«ç³»ç»Ÿ (MVP)", layout="wide")
st.title("âš¡ é›·è¾¾è¾å°„æºæŒ‡çº¹è¯†åˆ«ç³»ç»Ÿ (MVPç‰ˆ)")

# ç¡¬ç¼–ç æ¨¡å‹è·¯å¾„ (ä¸ml_modelingæ¨¡å—ä¸­çš„DEFAULT_MODEL_DIRä¿æŒä¸€è‡´)
MODEL_SAVE_DIR = "./saved_models"
MODEL_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, "mvp_model.pkl")

# ä¸´æ—¶çš„æ–‡ä»¶å¤„ç†ç›®å½•
TEMP_DIR = "./temp_uploads"
if not os.path.exists(TEMP_DIR):
    os.makedirs(TEMP_DIR)

# --- 2. è¾…åŠ©å‡½æ•° ---
def save_uploaded_file(uploaded_file):
    """ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•ï¼Œè¿”å›æ–‡ä»¶è·¯å¾„"""
    temp_path = os.path.join(TEMP_DIR, uploaded_file.name)
    with open(temp_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    return temp_path

def cleanup_temp_file(file_path):
    """åˆ é™¤ä¸´æ—¶æ–‡ä»¶"""
    if os.path.exists(file_path):
        os.remove(file_path)

# --- 3. é¡µé¢å¯¼èˆª (ä¾§è¾¹æ ) ---
st.sidebar.title("å¯¼èˆª")
page = st.sidebar.radio("é€‰æ‹©åŠŸèƒ½", ["ğŸ¯ é¢„æµ‹ (Prediction)", "ğŸ‹ï¸ è®­ç»ƒ (Training)"])

# ==============================================================================
# --- é¡µé¢ä¸€ï¼šé¢„æµ‹ ---
# ==============================================================================
if page == "ğŸ¯ é¢„æµ‹ (Prediction)":
    st.header("ğŸ¯ è¾å°„æºèº«ä»½é¢„æµ‹")

    # 1. æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(MODEL_SAVE_PATH):
        st.error(f"è­¦å‘Šï¼šæœªæ‰¾åˆ°å·²è®­ç»ƒçš„æ¨¡å‹ï¼ (åº”ä½äº: {MODEL_SAVE_PATH})")
        st.info("è¯·å…ˆå» 'ğŸ‹ï¸ è®­ç»ƒ (Training)' é¡µé¢è®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚")
        st.stop()
    
    st.success(f"å·²åŠ è½½æ¨¡å‹: {MODEL_SAVE_PATH}")

    # 2. æ–‡ä»¶ä¸Šä¼ 
    uploaded_file = st.file_uploader("ä¸Šä¼ ä¸€ä¸ª .iq æˆ– .dat æ–‡ä»¶è¿›è¡Œé¢„æµ‹", type=["iq", "dat"])

    if uploaded_file is not None:
        st.write(f"å·²ä¸Šä¼ æ–‡ä»¶: `{uploaded_file.name}`")
        
        # 3. å¼€å§‹é¢„æµ‹
        if st.button("å¼€å§‹é¢„æµ‹"):
            with st.spinner('æ­£åœ¨å¤„ç†...'):
                temp_file_path = ""
                try:
                    # æ­¥éª¤ A: ä¿å­˜ä¸´æ—¶æ–‡ä»¶
                    temp_file_path = save_uploaded_file(uploaded_file)
                    
                    # æ­¥éª¤ B: åŠ è½½æ•°æ®
                    st.subheader("1. æ•°æ®åŠ è½½")
                    data_obj = load_iq_data(temp_file_path)
                    if data_obj:
                        st.write(f"ä¿¡å·é•¿åº¦: {len(data_obj['iq_data'])}, é‡‡æ ·ç‡: {data_obj['sampling_rate']/1e6} MHz")
                    else:
                        st.error("æ•°æ®åŠ è½½å¤±è´¥ï¼")
                        st.stop()

                    # æ­¥éª¤ C: æå–ç‰¹å¾
                    st.subheader("2. ç‰¹å¾æå–")
                    # MVPé˜¶æ®µï¼Œæˆ‘ä»¬åªä½¿ç”¨ 'power_spectrum' ç‰¹å¾
                    feature_obj = extract_features(data_obj, methods=['power_spectrum'])
                    if not feature_obj.empty:
                        st.dataframe(feature_obj)
                    else:
                        st.error("ç‰¹å¾æå–å¤±è´¥ï¼")
                        st.stop()

                    # æ­¥éª¤ D: æ‰§è¡Œé¢„æµ‹
                    st.subheader("3. é¢„æµ‹ç»“æœ")
                    prediction_list = predict(feature_obj, MODEL_SAVE_PATH)
                    if prediction_list:
                        st.json(prediction_list[0]) # æ˜¾ç¤ºç¬¬ä¸€ä¸ªï¼ˆä¹Ÿæ˜¯å”¯ä¸€ä¸€ä¸ªï¼‰ç»“æœ
                    else:
                        st.error("é¢„æµ‹æ‰§è¡Œå¤±è´¥ï¼")
                        st.stop()

                except Exception as e:
                    st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
                
                finally:
                    # æ­¥éª¤ E: æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    cleanup_temp_file(temp_file_path)
                    
            st.success("é¢„æµ‹å®Œæˆï¼")

# ==============================================================================
# --- é¡µé¢äºŒï¼šè®­ç»ƒ ---
# ==============================================================================
elif page == "ğŸ‹ï¸ è®­ç»ƒ (Training)":
    st.header("ğŸ‹ï¸ è®­ç»ƒæ–°æ¨¡å‹")
    st.info("""
    **é‡è¦æç¤º (MVPç‰ˆ):**
    1.  è¯·ä¸Šä¼ **å¤šä¸ª**ç”¨äºè®­ç»ƒçš„IQæ–‡ä»¶ã€‚
    2.  æ–‡ä»¶å‘½åå¿…é¡»ç¬¦åˆè§„èŒƒ: **`æ ‡ç­¾å_ä»»æ„å­—ç¬¦.iq`** (ä¾‹å¦‚: `DeviceA_001.iq`, `DeviceB_sample_02.dat`)
    3.  ç¨‹åºå°†è‡ªåŠ¨ä»æ–‡ä»¶åä¸­æå– `_` å‰çš„éƒ¨åˆ†ä½œä¸ºæ ‡ç­¾ã€‚
    """)

    # 1. æ–‡ä»¶ä¸Šä¼  (å¤šæ–‡ä»¶)
    uploaded_files = st.file_uploader("ä¸Šä¼ è®­ç»ƒæ•°æ®é›† (å¯å¤šé€‰)", accept_multiple_files=True, type=["iq", "dat"])

    if uploaded_files:
        st.write(f"æ€»å…±ä¸Šä¼ äº† {len(uploaded_files)} ä¸ªæ–‡ä»¶ã€‚")
        
        # 2. å¼€å§‹è®­ç»ƒ
        if st.button("å¼€å§‹è®­ç»ƒ"):
            if len(uploaded_files) < 2:
                st.error("è®­ç»ƒè‡³å°‘éœ€è¦2ä¸ªæ–‡ä»¶ã€‚")
                st.stop()

            all_features_list = []
            all_labels = []
            temp_files_to_clean = []
            
            with st.spinner(f'æ­£åœ¨å¤„ç† {len(uploaded_files)} ä¸ªæ–‡ä»¶... è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...'):
                progress_bar = st.progress(0)
                
                for i, file in enumerate(uploaded_files):
                    try:
                        # æ­¥éª¤ A: æå–æ ‡ç­¾
                        label = file.name.split('_')[0]
                        if not label:
                            st.warning(f"æ–‡ä»¶ {file.name} å‘½åä¸è§„èŒƒï¼Œå·²è·³è¿‡ã€‚")
                            continue
                        
                        # æ­¥éª¤ B: ä¿å­˜ä¸´æ—¶æ–‡ä»¶
                        temp_path = save_uploaded_file(file)
                        temp_files_to_clean.append(temp_path)
                        
                        # æ­¥éª¤ C: åŠ è½½æ•°æ®
                        data_obj = load_iq_data(temp_path)
                        if not data_obj:
                            st.warning(f"åŠ è½½ {file.name} å¤±è´¥ï¼Œå·²è·³è¿‡ã€‚")
                            continue
                            
                        # æ­¥éª¤ D: æå–ç‰¹å¾
                        feature_obj = extract_features(data_obj, methods=['power_spectrum'])
                        if feature_obj.empty:
                            st.warning(f"æå– {file.name} ç‰¹å¾å¤±è´¥ï¼Œå·²è·³è¿‡ã€‚")
                            continue
                            
                        all_features_list.append(feature_obj)
                        all_labels.append(label)
                        
                    except Exception as e:
                        st.warning(f"å¤„ç† {file.name} æ—¶å‡ºé”™: {e}ï¼Œå·²è·³è¿‡ã€‚")
                    
                    progress_bar.progress((i + 1) / len(uploaded_files))

            # æ­¥éª¤ E: åˆå¹¶æ‰€æœ‰ç‰¹å¾
            if not all_features_list:
                st.error("æ²¡æœ‰æ–‡ä»¶è¢«æˆåŠŸå¤„ç†ï¼")
                st.stop()
                
            training_features = pd.concat(all_features_list, ignore_index=True)
            st.subheader("æå–çš„ç‰¹å¾æ€»è§ˆ (å‰5è¡Œ):")
            st.dataframe(training_features.head())
            st.write(f"æ€»å…±æå–äº† {len(all_labels)} ä¸ªæ ·æœ¬ã€‚")
            st.write(f"æ ‡ç­¾åˆ†å¸ƒ: {pd.Series(all_labels).value_counts().to_dict()}")

            # æ­¥éª¤ F: æ‰§è¡Œè®­ç»ƒ
            st.subheader("æ¨¡å‹è®­ç»ƒ")
            with st.spinner("æ­£åœ¨è®­ç»ƒæ¨¡å‹..."):
                model_path, train_log = train(training_features, all_labels, 
                                              model_type="mvp_logistic", params={})
            
            st.success(f"è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
            st.json(train_log)

            # æ­¥éª¤ G: (å¯é€‰) åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œè¯„ä¼°
            st.subheader("è®­ç»ƒé›†è¡¨ç°")
            predictions_on_train = predict(training_features, model_path)
            eval_results = evaluate(predictions_on_train, all_labels)
            st.metric(label="è®­ç»ƒé›†å‡†ç¡®ç‡", value=f"{eval_results.get('accuracy', 0):.2%}")
            st.write("æ··æ·†çŸ©é˜µ:")
            st.dataframe(pd.DataFrame(eval_results.get('confusion_matrix'), 
                                     columns=eval_results.get('labels_in_matrix'), 
                                     index=eval_results.get('labels_in_matrix')))

            # æ­¥éª¤ H: æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
            for path in temp_files_to_clean:
                cleanup_temp_file(path)
            st.info("ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†ã€‚")